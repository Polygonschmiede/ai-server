# LocalAI Installer Configuration
# Copy this file to .env.local and customize as needed
# The installer will use these values if present

# --------------------------
# Core Settings
# --------------------------

# Installation mode: "gpu" or "cpu"
MODE=gpu

# Run installer without prompts
NONINTERACTIVE=false

# Path for AI models on host machine
MODELS_PATH=/opt/localai/models

# LocalAI installation directory
LOCALAI_DIR=/opt/localai

# --------------------------
# Network Settings
# --------------------------

# External port for LocalAI API/WebUI
LOCALAI_PORT=8080

# Server IP address (for status messages)
SERVER_IP=192.168.178.50

# Stay-Awake HTTP service port
STAY_AWAKE_PORT=9876

# Stay-Awake HTTP bind address (0.0.0.0 for all interfaces)
STAY_AWAKE_BIND=0.0.0.0

# --------------------------
# System Settings
# --------------------------

# System timezone
TIMEZONE=Europe/Berlin

# --------------------------
# Security Settings
# --------------------------

# Configure UFW firewall: true or false
CONFIGURE_FIREWALL=true

# SSH hardening: "true", "false", or "auto" (will prompt)
SSH_HARDEN=auto

# --------------------------
# Power Management
# --------------------------

# Enable auto-suspend watcher
ENABLE_AUTO_SUSPEND=true

# Enable stay-awake HTTP service
ENABLE_STAY_AWAKE=true

# Enable Wake-on-LAN
ENABLE_WOL=true

# Network interface for WOL (auto-detect if empty)
WOL_INTERFACE=

# Minutes of idle time before auto-suspend
WAIT_MINUTES=30

# CPU idle threshold percentage for suspend
CPU_IDLE_THRESHOLD=90

# Maximum GPU utilization percentage for idle state
GPU_USAGE_MAX=10

# Maximum allowed GPU compute processes for idle state
GPU_PROC_FORBID=1

# Auto-suspend check interval in seconds
CHECK_INTERVAL=60

# --------------------------
# LLM Service Ports
# --------------------------
# Ports monitored for active LLM connections (space-separated)
PORTS_DEFAULT_STRING="8080 11434 8000 8081 7860 9600 5000 3000"
